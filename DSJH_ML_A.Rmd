---
title: "DSJH_ML_A"
author: "N.J. Blume"
date: "Saturday, January 24, 2015"
output: html_document
---

The data come from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants who performed barbell lifts correctly and incorrectly in 5 different ways (http://groupware.les.inf.puc-rio.br/har). The Variable "classe" describes the participant's form: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). First, we load the data and some libraries. 
```{r echo=FALSE}
library(caret); library(randomForest); #activate only on first run
setwd("~/..")
setwd("Desktop/LIFE/Courses/Coursera/DSJH/8_ML/Assignment")
training = read.csv("pml-training.csv")  # pre-partitioned training data 
testing = read.csv("pml-testing.csv")  # pre-partitioned testing data 
```

Number of observations per classe:
```{r echo=FALSE}
table(training$classe)
```

##Pre-processing 

Before pre-processing, the number of candidate predictors is:
```{r echo=FALSE}
dim(training)[2] - 1
```

Predictors that identify the person or the timestamp are excluded since I plan to generalize the model beyond 6 unique individuals (even though the testing data are specific to them; in this way I go beyond the assignment) but I don't plan to extract time of day or day of week, etc., or to model lower within-subject variance.
```{r}
training <- training[ , 8:160]
testing <- testing[ , 8:160]
```

For each remaining predictor, missing values in the training set are replaced with the median (if numeric). Predictors with too much missing data to compute a median in the training set are excluded from both the training and the testing sets. 
```{r}
#Note: I could remove trials where every value is NA
d2<-training
removeThisVar <- vector(mode="numeric", length=0)
for (pred in 1:dim(d2)[2]-1) {
    if(is.numeric(d2[ , pred])) {
        centraltendency <- median(d2[ , pred])
    } else {
        #print("need to calculate mode")
        centraltendency<-NA
    }
    if(is.na(centraltendency)) {
       removeThisVar <- c(removeThisVar, pred)
    } else {
        NAindex <- which(is.na(d2[ , pred]))
        if(length(NAindex) > 0) {
            for (i in 1:length(NAindex)) {
                d2[NAindex[i], pred] <- centraltendency
            }
        }
    }
}
filteredDescr <- d2[ , -removeThisVar]
filteredTesting <- testing[ , -removeThisVar]
```

More pre-processing to narrow the field of predictors would improve the random forest process. I could have pruned more variables by looking for ones with near zero variance, colinear ones, correlated ones, or redundantly predictive ones (with PCA) but I kept pre-processing to a minimum. After exclusion, the number of predictors is:
```{r echo=FALSE}
dim(filteredDescr)[2] - 1
filteredTraining <- filteredDescr
```

##Random Forest and Prediction Accuracy

I use a random forest technique to develop accurate predictions based on the performance of several decision trees. **The final model has an error rate of ~0.3% on the training set, i.e. an accuracy of above 99%.**

```{r}
modFit_RF <- randomForest(classe ~ ., data=filteredTraining, importance = TRUE)
modFit_RF
```

The confusion matrix (on the training set) for the Random Forest classifier is:
```{r}
modFit_RF$confusion
```

Predictions for the testing set are these:
```{r}
Prediction <- predict(modFit_RF, filteredTesting)
Prediction
#write.csv(predict, file = "firstforest.csv", row.names = FALSE)
```

**With Random Forest, additional cross valisation is uneccesary. Over-fitting is controled with internal out of the bag cross-validation steps and with averaging across trees.**

**I expect the out of sample (ie testing) error rate to be close to (though a little worse than) the error rate on the training sample.**
##Interpretation

Predictions have been made, but the relationship between the variables and the outcomes is opaque. Next, I look at which predictors matter most, I generate a single tree with these variables, and I relate the tree tothe original operational definition of classe values. 

The following lists the best predictors in order of importance.
```{r}
varImpPlot(modFit_RF)
```

The rfcv function helps better see the point when having more predictors barely increases information gain. 
```{r}
lastPredColumn <- dim(filteredTraining)[2] - 1
result <- rfcv(filteredTraining[1:lastPredColumn], filteredTraining$classe, cv.fold=5, step=0.8)
with(result, plot(n.var, error.cv, log="x", type="o", lwd=2))
result$error.cv
```

A set of 9 variables produces an acceptable error rate. Gains in accuracy above that number are negligible. The 9 most important variables were shown in an earlier graph. I now create new training and testing sets that are reduced to those 9 variables plus the outcome. I could run random forest again and generate a new predictive model with lower accuracy on the training set but less over-fitting and better accuracy on the testing set. I won't just to keep the assignment simple. I can however use these reduced data sets to perform one last task: generating an interpretable dendogram.

```{r}
smTraining <- filteredTraining[ , c("yaw_belt", "roll_belt", "pitch_belt", "magnet_dumbbell_z", "gyros_arm_y", "magnet_dumbbell_y", "pitch_forearm", "accel_dumbbell_y", "magnet_forearm_z", "classe")]
smTesting <- filteredTesting[ , c("yaw_belt", "roll_belt", "pitch_belt", "magnet_dumbbell_z", "gyros_arm_y", "magnet_dumbbell_y", "pitch_forearm", "accel_dumbbell_y", "magnet_forearm_z")]
```

I train a single decision tree on the 9 variables and I print its dendogram. 
```{r}
modFit <- train(classe ~ .,method="rpart",data=smTraining)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
predict(modFit,newdata=smTesting)
modFit
```

The accuracy of this tree is only 0.535, much lower than with the random forrest. It may also be overfit. But given how predictors were reduced the tree is still informative about the relationship between the variables, cut off values and outcome. For instance here the dendogram reflects the purposeful way in which participants were asked to carry out the exercises: for example, the first fork is a measure of roll_belt -- recall that participants were asked to throw their hips to the front (Class E). And so on.
